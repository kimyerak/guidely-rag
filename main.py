"""
rag_server.py
==============
FastAPI RAG ì„œë²„ â€“ ì „ì‹œíšŒìš© ì¸í„°ë™í‹°ë¸Œ ìŒì„± ì±—ë´‡
ìœ ì €ì˜ ì±„íŒ… ë©”ì‹œì§€ë¥¼ ë°›ì•„ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import os
import tempfile
from typing import List
from urllib.parse import urlparse

import fitz  # PyMuPDF
import requests
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from fastapi import FastAPI
from fastapi import Depends
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate
from langchain.schema import Document
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_openai import ChatOpenAI
# (torchvision ì—°ì‚°ì ëˆ„ë½ ì˜¤ë¥˜ ë°©ì§€ë¥¼ ìœ„í•´) transformers ê°€ torchvision ì„ import í•˜ì§€ ì•Šë„ë¡ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
os.environ["DISABLE_TORCHVISION_IMPORTS"] = "1"
from langchain_community.embeddings import SentenceTransformerEmbeddings
from sentence_transformers import CrossEncoder
from pydantic import BaseModel, Field
from config import URLS  # URL ëª©ë¡ë§Œ ë‹´ê³  ìˆëŠ” ëª¨ë“ˆ
from characters import CHARACTER_STYLE  # ìºë¦­í„° ìŠ¤íƒ€ì¼ ì •ì˜
from database import test_connection  # DB ì—°ê²° í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
import textwrap
import logging

# ---------------------------------------------------------------------------
# ë¡œê¹… ì„¤ì • (ëª¨ë“ˆ ìƒë‹¨ í•œ ë²ˆë§Œ)
# ---------------------------------------------------------------------------
logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s [%(levelname)s] %(message)s",
)

logger = logging.getLogger(__name__)

load_dotenv()  # OPENAI_API_KEY ë“± í™˜ê²½ ë³€ìˆ˜ ì½ê¸°

# ---------------------------------------------------------------------------
# FastAPI ì¸ìŠ¤í„´ìŠ¤
# ---------------------------------------------------------------------------

app = FastAPI(
    title="Guidely RAG Chatbot API",
    description="ì „ì‹œíšŒìš© ì¸í„°ë™í‹°ë¸Œ ìŒì„± ì±—ë´‡ RAG ì„œë²„",
    version="1.0.0",
    docs_url="/docs",  # Swagger UI ê²½ë¡œ
    redoc_url="/redoc",  # ReDoc ê²½ë¡œ
    openapi_url="/openapi.json"  # OpenAPI ìŠ¤í‚¤ë§ˆ ê²½ë¡œ
)

# ---------------------------------------------------------------------------
# ë²¡í„°ìŠ¤í† ì–´ ìœ í‹¸
# ---------------------------------------------------------------------------

VECTORSTORE_PATH = "faiss_index"
vectorstore: FAISS | None = None  # ì „ì—­ ë²¡í„°ìŠ¤í† ì–´ ê°ì²´
cross_encoder: CrossEncoder | None = None  # ì „ì—­ Cross-Encoder ê°ì²´

def load_pdf_from_url(url: str) -> Document:
    """PDF URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ Documentë¡œ ë°˜í™˜ (ë””ë²„ê¹… ë¡œê·¸ í¬í•¨)"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()

        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(response.content)
            tmp_file_path = tmp_file.name

        try:
            doc = fitz.open(tmp_file_path)
            text_content = ""
            total_chars = 0

            for page in doc:
                raw_text = page.get_text().strip()
                cleaned_text = raw_text  # í•„ìš”í•˜ë©´ clean_pdf_text(raw_text)

                if cleaned_text:
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # ğŸ“Œ â‘  í˜ì´ì§€ë³„ ê¸¸ì´Â·ì•ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    logger.info(
                        "Page %-3d | %5d chars | Preview: %s",
                        page.number + 1,
                        len(cleaned_text),
                        textwrap.shorten(cleaned_text, width=80, placeholder=" â€¦"),
                    )

                    text_content += f"í˜ì´ì§€ {page.number + 1}: {cleaned_text}\n\n"
                    total_chars += len(cleaned_text)

            

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ğŸ“Œ â‘¡ ì „ì²´ ì¶”ì¶œ ê²°ê³¼ ìš”ì•½
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            logger.info(
                "Finished extracting PDF (%s) â†’ total %d chars across %d pages",
                url,
                total_chars,
                len(doc),
            )
            doc.close() 
            # arXiv ë…¼ë¬¸ì´ë©´ ì¶œì²˜ ì£¼ì„ ì¶”ê°€
            if "arxiv.org" in url.lower():
                text_content = f"arXiv ë…¼ë¬¸ ì¶œì²˜: {url}\n\n" + text_content

            return Document(page_content=text_content,
                            metadata={"source": url, "type": "pdf"})

        finally:
            os.unlink(tmp_file_path)

    except Exception as e:
        logger.exception("[ERROR] PDF %s ë¡œë”© ì‹¤íŒ¨: %s", url, e)
        return Document(page_content="",
                        metadata={"source": url, "type": "pdf"})

def load_namu_page(url: str) -> Document:
    """ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ ë³¸ë¬¸ì„ Document ë¡œ ë¡œë“œ"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        content_div = soup.find("main") or soup
        content = content_div.get_text(separator="\n", strip=True)
        return Document(page_content=content, metadata={"source": url, "type": "namu_wiki"})
    except Exception as e:
        print(f"[ERROR] {url} ë¡œë”© ì‹¤íŒ¨: {e}")
        return Document(page_content="", metadata={"source": url, "type": "namu_wiki"})


def load_document_from_url(url: str) -> Document:
    """URL íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ë¡œë”ë¥¼ ì„ íƒí•˜ì—¬ Document ë°˜í™˜"""
    parsed_url = urlparse(url)
    
    # PDF íŒŒì¼ì¸ì§€ í™•ì¸
    if url.lower().endswith('.pdf') or 'arxiv.org/pdf/' in url.lower():
        return load_pdf_from_url(url)
    # ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ì¸ì§€ í™•ì¸
    elif 'namu.wiki' in parsed_url.netloc:
        return load_namu_page(url)
    else:
        # ê¸°ë³¸ì ìœ¼ë¡œ ì›¹ í˜ì´ì§€ë¡œ ì²˜ë¦¬
        return load_namu_page(url)


def chunk_documents(docs: List[Document], chunk_size: int = 1000):
    """ë¬¸ì„œ ë°°ì—´ì„ chunk_size ë‹¨ìœ„ë¡œ yield"""
    for i in range(0, len(docs), chunk_size):
        yield docs[i : i + chunk_size]


def create_faiss_vectorstore(
    docs: List[Document], embeddings, batch_size: int = 500
) -> FAISS:
    """ë°°ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë¬¸ì„œë¥¼ FAISS ìŠ¤í† ì–´ë¡œ ì¸ë±ì‹±"""
    vector_store: FAISS | None = None
    doc_count = 0
    for chunked_docs in chunk_documents(docs, batch_size):
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ“Œ ê° ì²­í¬ ì„ë² ë”© ì§„í–‰ ìƒí™© ë¡œê¹…
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info(f"ì„ë² ë”© ì¤‘: ë¬¸ì„œ {doc_count + 1} ~ {doc_count + len(chunked_docs)}")

        # PDF ë¬¸ì„œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€, ìˆë‹¤ë©´ ëª‡ ê°œì¸ì§€ ë¡œê¹…
        pdf_docs_in_chunk = [doc for doc in chunked_docs if doc.metadata.get("type") == "pdf"]
        if pdf_docs_in_chunk:
            logger.info(f"  â”” ì´ ì²­í¬ì— PDF ë¬¸ì„œ {len(pdf_docs_in_chunk)}ê°œ í¬í•¨:")
            for i, pdf_doc in enumerate(pdf_docs_in_chunk[:3]): # ì²˜ìŒ 3ê°œ PDF ì†ŒìŠ¤ë§Œ ë¡œê¹… (ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ)
                logger.info(f"    PDF [{i+1}] ì†ŒìŠ¤: {pdf_doc.metadata.get('source', 'N/A')}, ë‚´ìš© ì¼ë¶€: {textwrap.shorten(pdf_doc.page_content, width=50, placeholder='...')}")
        
        partial = FAISS.from_documents(chunked_docs, embeddings)
        if vector_store is None:
            vector_store = partial
        else:
            vector_store.merge_from(partial)
        doc_count += len(chunked_docs)
    logger.info(f"ì´ {doc_count}ê°œ ë¬¸ì„œ ì²­í¬ ì„ë² ë”© ì™„ë£Œ.")
    return vector_store


# ---------------------------------------------------------------------------
# FastAPI â€“ ì• í”Œë¦¬ì¼€ì´ì…˜ ì‹œì‘ ì‹œ ë²¡í„°ìŠ¤í† ì–´ ì¤€ë¹„
# ---------------------------------------------------------------------------

@app.on_event("startup")
def startup_event():
    global vectorstore, cross_encoder
    embeddings = SentenceTransformerEmbeddings(
        model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2",
        model_kwargs={"device": "cpu"},
    )

    print("===== ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ =====")
    test_connection()

    print("===== Cross-encoder ëª¨ë¸ ë¡œë“œ ì¤‘ =====")
    cross_encoder = CrossEncoder("cross-encoder/ms-marco-MiniLM-L-6-v2")
    print("===== Cross-encoder ëª¨ë¸ ë¡œë“œ ì™„ë£Œ =====")

    if os.path.exists(VECTORSTORE_PATH):
        print("===== ê¸°ì¡´ ë²¡í„°ìŠ¤í† ì–´ ë¡œë“œ ì¤‘ =====")
        vectorstore = FAISS.load_local(
            VECTORSTORE_PATH,
            embeddings,
            allow_dangerous_deserialization=True,
        )
    else:
        print("===== ë²¡í„°ìŠ¤í† ì–´ ìƒˆë¡œ ìƒì„± ì¤‘ =====")
        # 1) URL â†’ HTML â†’ Document
        documents = [load_document_from_url(url) for url in URLS]
        documents = [doc for doc in documents if doc.page_content.strip()]
        # 2) Document â†’ ì‘ì€ ì¡°ê°
        splitter = RecursiveCharacterTextSplitter(
            chunk_size=1200, chunk_overlap=200
        )
        docs = splitter.split_documents(documents)
        # 3) FAISS ì¸ë±ì‹±
        vectorstore = create_faiss_vectorstore(docs, embeddings)
        # 4) ë””ìŠ¤í¬ ì €ì¥
        vectorstore.save_local(VECTORSTORE_PATH)
        print("===== ë²¡í„°ìŠ¤í† ì–´ ìƒì„± ë° ì €ì¥ ì™„ë£Œ =====")


# ---------------------------------------------------------------------------
# ìš”ì²­/ì‘ë‹µ ëª¨ë¸
# ---------------------------------------------------------------------------

class ChatRequest(BaseModel):
    message: str = Field(..., description="ìœ ì €ì˜ ì±„íŒ… ë©”ì‹œì§€", example="ì•ˆë…•í•˜ì„¸ìš”! ì¼€ì´íŒì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”")
    character: str = Field(default="rumi", description="ì±—ë´‡ í˜ë¥´ì†Œë‚˜", example="rumi")

    class Config:
        schema_extra = {
            "example": {
                "message": "ì•ˆë…•í•˜ì„¸ìš”! ì¼€ì´íŒì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                "character": "rumi"
            }
        }


class ChatResponse(BaseModel):
    response: str = Field(..., description="ì±—ë´‡ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€")
    sources: List[dict] = Field(..., description="ì°¸ê³ í•œ ë¬¸ì„œ ì¶œì²˜")

    class Config:
        schema_extra = {
            "example": {
                "response": "ì•ˆë…•í•˜ì„¸ìš”! ì¼€ì´íŒì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš”. ì¼€ì´íŒì€...",
                "sources": [
                    {
                        "source": "https://namu.wiki/w/ì¼€ì´íŒ",
                        "content": "ì¼€ì´íŒ ê´€ë ¨ ë‚´ìš©..."
                    }
                ]
            }
        }


# ---------------------------------------------------------------------------
# ëŒ€í™” ìš”ì•½ APIìš© ëª¨ë¸
# ---------------------------------------------------------------------------

class ConversationMessage(BaseModel):
    role: str = Field(..., description="ë©”ì‹œì§€ ì—­í•  (user, assistant, system)")
    content: str = Field(..., description="ë©”ì‹œì§€ ë‚´ìš©")
    timestamp: str = Field(None, description="ë©”ì‹œì§€ ì‹œê°„ (ì„ íƒì‚¬í•­)")

class ConversationSummaryRequest(BaseModel):
    session_id: str = Field(..., description="ëŒ€í™” ì„¸ì…˜ ID")
    messages: List[ConversationMessage] = Field(..., description="ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸")
    count: int = Field(default=10, description="ìƒì„±í•  ìš”ì•½ ë¬¸ì¥ ê°œìˆ˜", ge=1, le=20)

    class Config:
        schema_extra = {
            "example": {
                "session_id": "550e8400-e29b-41d4-a716-446655440000",
                "messages": [
                    {
                        "role": "user",
                        "content": "ì•ˆë…•í•˜ì„¸ìš”! ì¸ìƒì£¼ì˜ì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
                        "timestamp": "2024-01-01T10:00:00Z"
                    },
                    {
                        "role": "assistant", 
                        "content": "ì•ˆë…•í•˜ì„¸ìš”! ì¸ìƒì£¼ì˜ëŠ” 19ì„¸ê¸° í›„ë°˜ í”„ë‘ìŠ¤ì—ì„œ ì‹œì‘ëœ ë¯¸ìˆ  ìš´ë™ì…ë‹ˆë‹¤...",
                        "timestamp": "2024-01-01T10:00:30Z"
                    }
                ],
                "count": 10
            }
        }

class ConversationSummaryResponse(BaseModel):
    session_id: str = Field(..., description="ëŒ€í™” ì„¸ì…˜ ID")
    total_messages: int = Field(..., description="ì´ ë©”ì‹œì§€ ê°œìˆ˜")
    summaries: List[str] = Field(..., description="ëŒ€í™” ìš”ì•½ ë¬¸ì¥ë“¤")
    
    class Config:
        schema_extra = {
            "example": {
                "session_id": "550e8400-e29b-41d4-a716-446655440000",
                "total_messages": 12,
                "summaries": [
                    "ê¹Œì¹˜ ë¬¸ì–‘, ì „í†µ ì†ì—ì„œ ë˜ì‚´ì•„ë‚œ ë‚ ê°œì§“",
                    "ë¯¸ë¼ê°€ ë˜ì§„ ì§ˆë¬¸ì— ìƒìƒë ¥ì´ ëì—†ì´ ë²ˆì ¸ê°”ì–´",
                    "ëª¨ë„¤ì˜ ë¶“ëì—ì„œ í”¼ì–´ë‚œ ë¹›ì˜ í–¥ì—°ì„ ë§Œë‚¬ê¸¸",
                    "ì¸ìƒì£¼ì˜ í™”ê°€ë“¤ì˜ í˜ì‹ ì ì¸ ì‹œì„ ì„ ë°œê²¬í•˜ë‹¤"
                ]
            }
        }


# ---------------------------------------------------------------------------
# ì—”ë“œí¬ì¸íŠ¸: /rag
# ---------------------------------------------------------------------------

@app.post("/rag/query", response_model=ChatResponse, tags=["RAG"])
async def generate_chat_response(req: ChatRequest):
    """
    ì „ì‹œíšŒìš© ì¸í„°ë™í‹°ë¸Œ ìŒì„± ì±—ë´‡ API
    
    ìœ ì €ì˜ ë©”ì‹œì§€ë¥¼ ë°›ì•„ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¸ê³ í•˜ì—¬ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    **Parameters:**
    - **message**: ìœ ì €ì˜ ì±„íŒ… ë©”ì‹œì§€
    - **character**: ì±—ë´‡ í˜ë¥´ì†Œë‚˜ (rumi, mira, zoey, jinu)
    
    **Returns:**
    - **response**: ì±—ë´‡ì˜ ìì—°ìŠ¤ëŸ¬ìš´ ë‹µë³€
    - **sources**: ì°¸ê³ í•œ ë¬¸ì„œ ì¶œì²˜ (ìˆëŠ” ê²½ìš°)
    
    **Example Request:**
    ```json
    {
        "message": "ì•ˆë…•í•˜ì„¸ìš”! ì¼€ì´íŒì— ëŒ€í•´ ì•Œë ¤ì£¼ì„¸ìš”",
        "character": "rumi"
    }
    ```
    
    **Example Response:**
    ```json
    {
        "response": "ì•ˆë…•í•˜ì„¸ìš”! ì¼€ì´íŒì— ëŒ€í•´ ê¶ê¸ˆí•˜ì‹œêµ°ìš”. ì¼€ì´íŒì€...",
        "sources": [
            {
                "source": "https://namu.wiki/w/ì¼€ì´íŒ",
                "content": "ì¼€ì´íŒ ê´€ë ¨ ë‚´ìš©..."
            }
        ]
    }
    ```
    """
    global vectorstore, cross_encoder
    if vectorstore is None or cross_encoder is None:
        return {"error": "Vectorstore or CrossEncoder not initialized."}

    user_message = req.message
    character = req.character
    logger.info(f"User message: {user_message}")
    logger.info(f"Character: {character}")

    # ìºë¦­í„° ìŠ¤íƒ€ì¼ ê°€ì ¸ì˜¤ê¸°
    char_style = CHARACTER_STYLE[character]

    # ----------------- 1) ë¬¸ë§¥ ê²€ìƒ‰ (2-stage: Retriever + Reranker) -----------------
    # 1-1) 1ì°¨ ê²€ìƒ‰ (Retriever): FAISSì—ì„œ Top-100 ë¬¸ì„œ ê°€ì ¸ì˜¤ê¸°
    logger.info("1ë‹¨ê³„: FAISSì—ì„œ ê´€ë ¨ ë¬¸ì„œ 10ê°œ ê²€ìƒ‰")
    retriever = vectorstore.as_retriever(search_kwargs={"k": 10})  # 100 â†’ 10
    retrieved_docs = retriever.get_relevant_documents(user_message)
    logger.info(f"  -> {len(retrieved_docs)}ê°œ ë¬¸ì„œ ê²€ìƒ‰ ì™„ë£Œ.")

    # 1-2) 2ì°¨ ê²€ìƒ‰ (Reranker): Cross-encoderë¡œ ê´€ë ¨ì„± ë†’ì€ Top-3 ë¬¸ì„œ ì¬ì„ ì •
    logger.info("2ë‹¨ê³„: Cross-encoderë¡œ ì¬ì ìˆ˜í™”í•˜ì—¬ Top-3 ì„ ì •")
    # ì¿¼ë¦¬ì™€ ë¬¸ì„œ ë‚´ìš©ìœ¼ë¡œ í˜ì–´ ìƒì„±
    pairs = [[user_message, doc.page_content] for doc in retrieved_docs]

    # Cross-encoderë¡œ ì ìˆ˜ ê³„ì‚°
    scores = cross_encoder.predict(pairs, show_progress_bar=True)

    # ì ìˆ˜ì™€ ë¬¸ì„œë¥¼ íŠœí”Œë¡œ ë¬¶ì–´ ì ìˆ˜ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    scored_docs = sorted(zip(scores, retrieved_docs), key=lambda x: x[0], reverse=True)

    # ìƒìœ„ 3ê°œ ë¬¸ì„œ ì„ íƒ
    docs = [doc for score, doc in scored_docs[:3]]
    logger.info("  -> ì¬ì ìˆ˜í™” í›„ Top-3 ë¬¸ì„œ ì„ ì • ì™„ë£Œ.")
    for i, doc in enumerate(docs):
        logger.info(f"    Top {i+1}: {doc.metadata.get('source', 'N/A')} (Score: {scored_docs[i][0]:.4f})")
    
    context = "\n\n".join(d.page_content for d in docs)

    # ----------------- 2) í”„ë¡¬í”„íŠ¸ -----------------
    prompt = PromptTemplate(
        input_variables=["message", "context", "character", "char_style"],
        template=(
            "ë‹¹ì‹ ì€ ì „ì‹œíšŒì—ì„œ ë°©ë¬¸ê°ê³¼ ìì—°ìŠ¤ëŸ½ê²Œ ëŒ€í™”í•˜ëŠ” ì¹œê·¼í•œ ì±—ë´‡ì…ë‹ˆë‹¤.\n\n"
            
            "## ì£¼ì–´ì§„ ì •ë³´\n"
            "**ìœ ì € ë©”ì‹œì§€:**\n{message}\n\n"
            "**ì°¸ê³  ìë£Œ (ê´€ë ¨ ë¬¸ì„œ):**\n{context}\n\n"
            
            "## ìºë¦­í„° ì„¤ì •\n"
            "- **ì´ë¦„**: {char_style[name]}\n"
            "  - **ì„±ê²©**: {char_style[style]}\n"
            "  - **ë§íˆ¬**: {char_style[voice]}\n"
            "  - **ì˜ˆì‹œ**: \"{char_style[example]}\"\n\n"
            
            "## ì„ë¬´\n"
            "ìœ ì €ì˜ ë©”ì‹œì§€ì— ëŒ€í•´ ìºë¦­í„°ì˜ ì„±ê²©ì— ë§ê²Œ ìì—°ìŠ¤ëŸ½ê³  ì¹œê·¼í•˜ê²Œ ë‹µë³€í•´ì£¼ì„¸ìš”.\n\n"
            
            "## ë‹µë³€ ì§€ì¹¨\n"
            "1. **ìì—°ìŠ¤ëŸ¬ìš´ ëŒ€í™”**: ì „ì‹œíšŒì—ì„œ ì‹¤ì œ ì‚¬ëŒê³¼ ëŒ€í™”í•˜ëŠ” ê²ƒì²˜ëŸ¼ ìì—°ìŠ¤ëŸ½ê²Œ\n"
            "2. **ìºë¦­í„° ìœ ì§€**: ì£¼ì–´ì§„ ìºë¦­í„°ì˜ ì„±ê²©ê³¼ ë§íˆ¬ë¥¼ ì¼ê´€ë˜ê²Œ ìœ ì§€\n"
            "3. **ì •ë³´ í™œìš©**: ì°¸ê³  ìë£Œê°€ ìˆë‹¤ë©´ ìì—°ìŠ¤ëŸ½ê²Œ í™œìš©í•˜ë˜, ì–µì§€ìŠ¤ëŸ½ì§€ ì•Šê²Œ\n"
            "4. **ì¹œê·¼í•¨**: ë°©ë¬¸ê°ì„ í™˜ì˜í•˜ê³  ë„ì›€ì´ ë˜ëŠ” ì •ë³´ ì œê³µ\n"
            "5. **ê°„ê²°í•¨**: ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ, ëŒ€í™”í•˜ê¸° ì¢‹ì€ ê¸¸ì´ë¡œ\n\n"
            
            "## ì£¼ì˜ì‚¬í•­\n"
            "- ë‹µë³€ë§Œ ì¶œë ¥í•˜ê³  ë‹¤ë¥¸ ì„¤ëª… ê¸ˆì§€\n"
            "- í•œêµ­ì–´ë¡œ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±\n"
            "- ì°¸ê³  ìë£Œê°€ ì—†ì–´ë„ ìºë¦­í„°ë‹µê²Œ ë‹µë³€\n"
            "- í™˜ê°ì´ë‚˜ ì¶”ì¸¡ì„± ì •ë³´ ì ˆëŒ€ ê¸ˆì§€"
        ),
    )


    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7)
    final_prompt = prompt.format(
        message=user_message,
        context=context,
        character=character,
        char_style=char_style
    )
    response = llm.invoke(final_prompt)

    # ----------------- 3) ì‘ë‹µ -----------------
    sources = [
        {
            "source": doc.metadata.get("source", "N/A"),
            "content": doc.page_content[:300],
        }
        for doc in docs
    ]
    logger.info(f"Generated response: {response.content.strip()}")
    return ChatResponse(
        response=response.content.strip(),
        sources=sources,
    )


# ---------------------------------------------------------------------------
# ì—”ë“œí¬ì¸íŠ¸: /conversation/summarize - ì—”ë”©í¬ë ˆë”§ìš© ëŒ€í™” ìš”ì•½
# ---------------------------------------------------------------------------

@app.post("/rag/summarize", response_model=ConversationSummaryResponse, tags=["RAG"])
async def summarize_conversation(req: ConversationSummaryRequest):
    """
    ì—”ë”©í¬ë ˆë”§ìš© ëŒ€í™” ìš”ì•½ API
    
    ëŒ€í™” ë‚´ìš©ì„ ë¶„ì„í•˜ì—¬ ê°ì„±ì ì´ê³  ì‹œì ì¸ í•œ ì¤„ ìš”ì•½ë“¤ì„ ìƒì„±í•©ë‹ˆë‹¤.
    
    **Parameters:**
    - **session_id**: ëŒ€í™” ì„¸ì…˜ ID
    - **messages**: ëŒ€í™” ë©”ì‹œì§€ ë¦¬ìŠ¤íŠ¸
    - **count**: ìƒì„±í•  ìš”ì•½ ë¬¸ì¥ ê°œìˆ˜ (1-20)
    
    **Returns:**
    - **session_id**: ëŒ€í™” ì„¸ì…˜ ID
    - **total_messages**: ì´ ë©”ì‹œì§€ ê°œìˆ˜
    - **summaries**: ê°ì„±ì ì¸ í•œ ì¤„ ìš”ì•½ë“¤
    """
    logger.info(f"ëŒ€í™” ìš”ì•½ ìš”ì²­: session_id={req.session_id}, ë©”ì‹œì§€ ìˆ˜={len(req.messages)}")
    
    # ëŒ€í™” ë‚´ìš©ì„ í•˜ë‚˜ì˜ í…ìŠ¤íŠ¸ë¡œ í•©ì¹˜ê¸°
    conversation_text = ""
    for msg in req.messages:
        conversation_text += f"{msg.role}: {msg.content}\n"
    
    logger.info(f"ëŒ€í™” í…ìŠ¤íŠ¸ ê¸¸ì´: {len(conversation_text)} ë¬¸ì")
    
    # GPTë¥¼ ì‚¬ìš©í•˜ì—¬ ê°ì„±ì ì¸ í•œ ì¤„ ìš”ì•½ë“¤ ìƒì„±
    llm = ChatOpenAI(model_name="gpt-4o", temperature=0.7)
    
    summary_prompt = f"""
ë‹¤ìŒì€ ì „ì‹œíšŒì—ì„œ ë°©ë¬¸ê°ê³¼ ì±—ë´‡ ê°„ì˜ ëŒ€í™”ì…ë‹ˆë‹¤. 
ì´ ëŒ€í™”ë¥¼ ë°”íƒ•ìœ¼ë¡œ ì—”ë”©í¬ë ˆë”§ìš© ê°ì„±ì ì´ê³  ì‹œì ì¸ í•œ ì¤„ ìš”ì•½ {req.count}ê°œë¥¼ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

**ëŒ€í™” ë‚´ìš©:**
{conversation_text}

**ìš”êµ¬ì‚¬í•­:**
1. ê° ìš”ì•½ì€ í•œ ë¬¸ì¥ ë˜ëŠ” í•œ êµ¬ì ˆë¡œ ê°„ê²°í•˜ê²Œ
2. ê°ì„±ì ì´ê³  ì‹œì ì¸ í‘œí˜„ ì‚¬ìš©
3. ë°©ë¬¸ê°ì´ ê²½í—˜í•œ ê°ë™ì´ë‚˜ ìƒˆë¡œìš´ ë°œê²¬ì„ ë‹´ì•„ë‚´ê¸°
4. ì „ì‹œíšŒì˜ ë¶„ìœ„ê¸°ì™€ ì˜ˆìˆ ì  ê°ì„±ì„ ë°˜ì˜
5. ê° ìš”ì•½ì€ ë…ë¦½ì ì´ë©´ì„œë„ ì „ì²´ì ìœ¼ë¡œ ì¡°í™”ë¡œì›Œì•¼ í•¨

**ì˜ˆì‹œ ìŠ¤íƒ€ì¼:**
- "ê¹Œì¹˜ ë¬¸ì–‘, ì „í†µ ì†ì—ì„œ ë˜ì‚´ì•„ë‚œ ë‚ ê°œì§“"
- "ë¯¸ë¼ê°€ ë˜ì§„ ì§ˆë¬¸ì— ìƒìƒë ¥ì´ ëì—†ì´ ë²ˆì ¸ê°”ì–´"
- "ëª¨ë„¤ì˜ ë¶“ëì—ì„œ í”¼ì–´ë‚œ ë¹›ì˜ í–¥ì—°ì„ ë§Œë‚¬ì–´"

**ì‘ë‹µ í˜•ì‹ (JSON):**
{{
    "summaries": [
        "ì²« ë²ˆì§¸ ê°ì„±ì ì¸ ìš”ì•½",
        "ë‘ ë²ˆì§¸ ê°ì„±ì ì¸ ìš”ì•½",
        "ì„¸ ë²ˆì§¸ ê°ì„±ì ì¸ ìš”ì•½"
    ]
}}

JSON í˜•ì‹ìœ¼ë¡œë§Œ ì‘ë‹µí•´ì£¼ì„¸ìš”:
"""
    
    try:
        response = llm.invoke(summary_prompt)
        
        # JSON íŒŒì‹±
        import json
        import re
        
        # JSON ë¶€ë¶„ë§Œ ì¶”ì¶œ (```json íƒœê·¸ ì œê±°)
        json_match = re.search(r'```json\s*(.*?)\s*```', response.content, re.DOTALL)
        if json_match:
            json_str = json_match.group(1)
        else:
            json_str = response.content.strip()
        
        summary_data = json.loads(json_str)
        summaries = summary_data.get("summaries", [])
        
        logger.info(f"ëŒ€í™” ìš”ì•½ ì™„ë£Œ: {len(summaries)}ê°œ ìš”ì•½ ìƒì„±")
        
        return ConversationSummaryResponse(
            session_id=req.session_id,
            total_messages=len(req.messages),
            summaries=summaries
        )
        
    except json.JSONDecodeError as e:
        logger.error(f"JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
        # íŒŒì‹± ì‹¤íŒ¨ì‹œ ê¸°ë³¸ ì‘ë‹µ
        return ConversationSummaryResponse(
            session_id=req.session_id,
            total_messages=len(req.messages),
            summaries=[
                "ì˜ë¯¸ìˆëŠ” ëŒ€í™” ì†ì—ì„œ ìƒˆë¡œìš´ ë°œê²¬ì„ í–ˆì–´",
                "ì „ì‹œíšŒì—ì„œ ë§Œë‚œ íŠ¹ë³„í•œ ìˆœê°„ë“¤"
            ]
        )
        
    except Exception as e:
        logger.error(f"ëŒ€í™” ìš”ì•½ ìƒì„± ì˜¤ë¥˜: {e}")
        raise


# ---------------------------------------------------------------------------
# í—¬ìŠ¤ì²´í¬ ì—”ë“œí¬ì¸íŠ¸
# ---------------------------------------------------------------------------

@app.get("/health", tags=["System"])
async def health_check():
    """RAG ì„œë¹„ìŠ¤ í—¬ìŠ¤ì²´í¬"""
    return {
        "status": "healthy",
        "service": "Guidely RAG Service",
        "vectorstore_ready": vectorstore is not None,
        "cross_encoder_ready": cross_encoder is not None
    }
"""
ë¬¸ì„œ ë¡œë” ìœ í‹¸ë¦¬í‹°
"""
import os
import tempfile
from typing import List
from urllib.parse import urlparse

import fitz  # PyMuPDF
import requests
from bs4 import BeautifulSoup
from langchain.schema import Document
import textwrap
import logging

logger = logging.getLogger(__name__)


def load_pdf_from_url(url: str) -> Document:
    """PDF URLì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ì—¬ Documentë¡œ ë°˜í™˜ (ë””ë²„ê¹… ë¡œê·¸ í¬í•¨)"""
    try:
        response = requests.get(url, timeout=30)
        response.raise_for_status()

        with tempfile.NamedTemporaryFile(delete=False, suffix=".pdf") as tmp_file:
            tmp_file.write(response.content)
            tmp_file_path = tmp_file.name

        try:
            doc = fitz.open(tmp_file_path)
            text_content = ""
            total_chars = 0

            for page in doc:
                raw_text = page.get_text().strip()
                cleaned_text = raw_text  # í•„ìš”í•˜ë©´ clean_pdf_text(raw_text)

                if cleaned_text:
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    # ğŸ“Œ â‘  í˜ì´ì§€ë³„ ê¸¸ì´Â·ì•ë¶€ë¶„ ë¯¸ë¦¬ë³´ê¸° ì¶œë ¥
                    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
                    logger.info(
                        "Page %-3d | %5d chars | Preview: %s",
                        page.number + 1,
                        len(cleaned_text),
                        textwrap.shorten(cleaned_text, width=80, placeholder=" â€¦"),
                    )

                    text_content += f"í˜ì´ì§€ {page.number + 1}: {cleaned_text}\n\n"
                    total_chars += len(cleaned_text)

            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            # ğŸ“Œ â‘¡ ì „ì²´ ì¶”ì¶œ ê²°ê³¼ ìš”ì•½
            # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            logger.info(
                "Finished extracting PDF (%s) â†’ total %d chars across %d pages",
                url,
                total_chars,
                len(doc),
            )
            doc.close() 
            # arXiv ë…¼ë¬¸ì´ë©´ ì¶œì²˜ ì£¼ì„ ì¶”ê°€
            if "arxiv.org" in url.lower():
                text_content = f"arXiv ë…¼ë¬¸ ì¶œì²˜: {url}\n\n" + text_content

            return Document(page_content=text_content,
                            metadata={"source": url, "type": "pdf"})

        finally:
            os.unlink(tmp_file_path)

    except Exception as e:
        logger.exception("[ERROR] PDF %s ë¡œë”© ì‹¤íŒ¨: %s", url, e)
        return Document(page_content="",
                        metadata={"source": url, "type": "pdf"})


def load_namu_page(url: str) -> Document:
    """ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ ë³¸ë¬¸ì„ Document ë¡œ ë¡œë“œ"""
    try:
        response = requests.get(url, timeout=10)
        response.raise_for_status()
        soup = BeautifulSoup(response.text, "html.parser")
        content_div = soup.find("main") or soup
        content = content_div.get_text(separator="\n", strip=True)
        return Document(page_content=content, metadata={"source": url, "type": "namu_wiki"})
    except Exception as e:
        print(f"[ERROR] {url} ë¡œë”© ì‹¤íŒ¨: {e}")
        return Document(page_content="", metadata={"source": url, "type": "namu_wiki"})


def load_document_from_url(url: str) -> Document:
    """URL íƒ€ì…ì— ë”°ë¼ ì ì ˆí•œ ë¡œë”ë¥¼ ì„ íƒí•˜ì—¬ Document ë°˜í™˜"""
    # ë¡œì»¬ íŒŒì¼ì¸ì§€ í™•ì¸
    if not url.startswith('http'):
        return load_local_file(url)
    
    parsed_url = urlparse(url)
    
    # PDF íŒŒì¼ì¸ì§€ í™•ì¸
    if url.lower().endswith('.pdf') or 'arxiv.org/pdf/' in url.lower():
        return load_pdf_from_url(url)
    # ë‚˜ë¬´ìœ„í‚¤ í˜ì´ì§€ì¸ì§€ í™•ì¸
    elif 'namu.wiki' in parsed_url.netloc:
        return load_namu_page(url)
    else:
        # ê¸°ë³¸ì ìœ¼ë¡œ ì›¹ í˜ì´ì§€ë¡œ ì²˜ë¦¬
        return load_namu_page(url)


def load_local_file(file_path: str) -> Document:
    """ë¡œì»¬ íŒŒì¼ì„ ë¡œë“œí•˜ì—¬ Documentë¡œ ë°˜í™˜"""
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        return Document(page_content=content, metadata={"source": file_path, "type": "local_file"})
    except Exception as e:
        logger.exception("[ERROR] ë¡œì»¬ íŒŒì¼ %s ë¡œë”© ì‹¤íŒ¨: %s", file_path, e)
        return Document(page_content="", metadata={"source": file_path, "type": "local_file"})

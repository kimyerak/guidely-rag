"""
ë²¡í„°ìŠ¤í† ì–´ ìœ í‹¸ë¦¬í‹°
"""
from typing import List
from langchain.schema import Document
from langchain_community.vectorstores import FAISS
from langchain.text_splitter import RecursiveCharacterTextSplitter
import textwrap
import logging

logger = logging.getLogger(__name__)


def chunk_documents(docs: List[Document], chunk_size: int = 1000):
    """ë¬¸ì„œ ë°°ì—´ì„ chunk_size ë‹¨ìœ„ë¡œ yield"""
    for i in range(0, len(docs), chunk_size):
        yield docs[i : i + chunk_size]


def create_faiss_vectorstore(
    docs: List[Document], embeddings, batch_size: int = 500
) -> FAISS:
    """ë°°ì¹˜ ì„ë² ë”©ìœ¼ë¡œ ëŒ€ìš©ëŸ‰ ë¬¸ì„œë¥¼ FAISS ìŠ¤í† ì–´ë¡œ ì¸ë±ì‹±"""
    vector_store: FAISS | None = None
    doc_count = 0
    for chunked_docs in chunk_documents(docs, batch_size):
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        # ğŸ“Œ ê° ì²­í¬ ì„ë² ë”© ì§„í–‰ ìƒí™© ë¡œê¹…
        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
        logger.info(f"ì„ë² ë”© ì¤‘: ë¬¸ì„œ {doc_count + 1} ~ {doc_count + len(chunked_docs)}")

        # PDF ë¬¸ì„œê°€ í¬í•¨ë˜ì–´ ìˆëŠ”ì§€, ìˆë‹¤ë©´ ëª‡ ê°œì¸ì§€ ë¡œê¹…
        pdf_docs_in_chunk = [doc for doc in chunked_docs if doc.metadata.get("type") == "pdf"]
        if pdf_docs_in_chunk:
            logger.info(f"  â”” ì´ ì²­í¬ì— PDF ë¬¸ì„œ {len(pdf_docs_in_chunk)}ê°œ í¬í•¨:")
            for i, pdf_doc in enumerate(pdf_docs_in_chunk[:3]): # ì²˜ìŒ 3ê°œ PDF ì†ŒìŠ¤ë§Œ ë¡œê¹… (ë„ˆë¬´ ë§ìœ¼ë©´ ìƒëµ)
                logger.info(f"    PDF [{i+1}] ì†ŒìŠ¤: {pdf_doc.metadata.get('source', 'N/A')}, ë‚´ìš© ì¼ë¶€: {textwrap.shorten(pdf_doc.page_content, width=50, placeholder='...')}")
        
        partial = FAISS.from_documents(chunked_docs, embeddings)
        if vector_store is None:
            vector_store = partial
        else:
            vector_store.merge_from(partial)
        doc_count += len(chunked_docs)
    logger.info(f"ì´ {doc_count}ê°œ ë¬¸ì„œ ì²­í¬ ì„ë² ë”© ì™„ë£Œ.")
    return vector_store
